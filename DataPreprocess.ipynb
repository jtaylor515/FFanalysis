{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeffreytaylor/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nbformat\n",
    "from io import StringIO\n",
    "import os\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape basic stat datasets from FantasyPros.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape overall scoring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of URLs to scrape for fantasy stats\n",
    "urls = [\n",
    "    'https://www.fantasypros.com/nfl/stats/qb.php?scoring=HALF&roster=y',\n",
    "    'https://www.fantasypros.com/nfl/stats/rb.php?scoring=HALF&roster=y',\n",
    "    'https://www.fantasypros.com/nfl/stats/wr.php?scoring=HALF&roster=y',\n",
    "    'https://www.fantasypros.com/nfl/stats/te.php?scoring=HALF&roster=y'\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "data_frames = []\n",
    "\n",
    "for url in urls:\n",
    "    # Send an HTTP GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find the table on the page\n",
    "        table = soup.find('table')\n",
    "\n",
    "        table_string = str(table)\n",
    "        table_io = StringIO(table_string)\n",
    "        # Read the table into a DataFrame\n",
    "        df = pd.read_html(table_io)[0]       \n",
    "        \n",
    "        # Add a \"LOC\" column to the DataFrame\n",
    "        loc = url.split('/')[-1][:2]\n",
    "        df[(\"LOC\", \"POS\")] = loc\n",
    "\n",
    "        data_frames.append(df)\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data from {url}\")\n",
    "\n",
    "# Merge all DataFrames into one based on the first and second row headers\n",
    "merged_df = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "# Combine values in column names (headers) and row 0\n",
    "merged_df.columns = merged_df.columns.map(' '.join)\n",
    "\n",
    "# Reset the index\n",
    "merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Rename columns as specified\n",
    "merged_df = merged_df.rename(columns={\"Unnamed: 0_level_0 Rank\": \"POS RANK\", \"Unnamed: 1_level_0 Player\": \"PLAYER\", \"LOC POS\": \"POS\"})\n",
    "\n",
    "merged_df.to_csv('datasets/overall_scoring.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape snap counts data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# List of URLs to scrape for snap counts\n",
    "snap_count_urls = [\n",
    "    'https://www.fantasypros.com/nfl/reports/snap-counts/rb.php?show=perc',\n",
    "    'https://www.fantasypros.com/nfl/reports/snap-counts/wr.php?show=perc',\n",
    "    'https://www.fantasypros.com/nfl/reports/snap-counts/te.php?show=perc'\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store DataFrames for snap counts\n",
    "snap_count_data_frames = []\n",
    "\n",
    "for url in snap_count_urls:\n",
    "    # Send an HTTP GET request to the URL for snap counts\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find the table on the page\n",
    "        table = soup.find('table')\n",
    "\n",
    "        html_content = str(table)\n",
    "        df = pd.read_html(StringIO(html_content), header=[0])[0]  \n",
    "        \n",
    "        # Add a \"POS\" column to the DataFrame for snap counts\n",
    "        pos = url.split('/')[-1][:2]\n",
    "        df[(\"POS\")] = pos\n",
    "\n",
    "        snap_count_data_frames.append(df)\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data from {url} (snap counts)\")\n",
    "\n",
    "# Concatenate (append) all DataFrames for snap counts\n",
    "snap_count_merged_df = pd.concat(snap_count_data_frames, ignore_index=True)\n",
    "\n",
    "# If you want to save the data to a CSV file, you can do it like this:\n",
    "snap_count_merged_df.to_csv('datasets/snap_counts.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape weekly historical scoring data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate URLs to scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get user input for the number of weeks to scrape\n",
    "num_weeks = int(input(\"Enter the previous week for data collection: \"))\n",
    "\n",
    "# Define the base URL and the page options\n",
    "base_url = \"https://www.fantasypros.com/nfl/stats/\"\n",
    "\n",
    "# List of page options\n",
    "pages = ['qb.php', 'wr.php', 'rb.php', 'te.php']\n",
    "\n",
    "# Initialize the list to store the generated URLs\n",
    "urls = []\n",
    "\n",
    "# Generate URLs based on user input for the 2020, 2021, 2022, and 2023 seasons\n",
    "for page in pages:\n",
    "    for year in range(2018, 2024):  # Loop through years 2020, 2021, 2022, and 2023\n",
    "        max_week = 18 if year > 2020 else 17  # Weeks 1-17 for 2020, Weeks 1-18 for 2021 and 2022\n",
    "        for week in range(1, max_week + 1):\n",
    "            if year == 2023 and week > num_weeks: # Weeks 1-INPUT for 2023\n",
    "                break  # Stop generating URLs for 2022 if the desired week is reached\n",
    "            url = f\"{base_url}{page}?year={year}&range=week&week={week}\"\n",
    "            urls.append(url)\n",
    "\n",
    "# # Print the list of generated URLs\n",
    "# for url in urls:\n",
    "#     print(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows: 39638\n",
      "Number of Columns: 33\n",
      "Rows times Columns: 1308054\n",
      "File Size: 5.00 MB\n"
     ]
    }
   ],
   "source": [
    "final_dataset = pd.DataFrame()\n",
    "\n",
    "# Iterate through the URLs\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Assuming the data is in a table, you may need to adjust the code based on the actual structure\n",
    "        table = soup.find('table')\n",
    "\n",
    "    \n",
    "        table_string = str(table)\n",
    "        table_io = StringIO(table_string)\n",
    "        # Read the table into a DataFrame\n",
    "        df = pd.read_html(table_io)[0]\n",
    "\n",
    "        # Add a \"LOC\" column to the DataFrame\n",
    "        loc = url.split('/')[-1][:2]\n",
    "        df[(\"LOC\", \"POS\")] = loc\n",
    "        \n",
    "        # Extract week value from the URL\n",
    "        week_value = int(url.split('week=')[1])\n",
    "        # Regular expression pattern to match the year\n",
    "        pattern = r'year=(\\d{4})'\n",
    "\n",
    "        # Use re.search to find the match\n",
    "        match = re.search(pattern, url)\n",
    "        year_value = match.group(1)\n",
    "        \n",
    "        # Add a new 'Week' column with the week value\n",
    "        df['WEEK'] = week_value\n",
    "        df['YEAR'] = int(year_value)\n",
    "        \n",
    "        # Concatenate the DataFrame to the final dataset\n",
    "        final_dataset = pd.concat([final_dataset, df], ignore_index=True)\n",
    "    else:\n",
    "        print(f\"Failed to fetch data from URL: {url}\")\n",
    "\n",
    "# Now, final_dataset contains the combined data with a 'Week' column\n",
    "final_dataset.head(10)\n",
    "\n",
    "# Combine values in column names (headers) and row 0\n",
    "final_dataset.columns = final_dataset.columns.map(' '.join)\n",
    "\n",
    "# Reset the index\n",
    "final_dataset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Rename columns as specified\n",
    "final_dataset = final_dataset.rename(columns={\"Unnamed: 0_level_0 Rank\": \"POS RANK\", \"Unnamed: 1_level_0 Player\": \"PLAYER\", \"LOC POS\": \"POS\", \"WEEK \": \"WEEK\", \"YEAR \": \"YEAR\"})\n",
    "\n",
    "# Create weighted on date column\n",
    "# Define a custom function to create a date\n",
    "def create_date(row):\n",
    "    year = row['YEAR']\n",
    "    week = row['WEEK']\n",
    "    day_of_week = 1  # You can choose a specific day of the week\n",
    "\n",
    "    # Create a date by considering the year, week, and day_of_week\n",
    "    date = pd.to_datetime(f'{year}-W{week}-{day_of_week}', format='%Y-W%U-%w')\n",
    "\n",
    "    return date\n",
    "\n",
    "# Apply the custom function to create the 'Date' column\n",
    "final_dataset['DATE'] = final_dataset.apply(create_date, axis=1)\n",
    "\n",
    "# Define your custom weight function, for example, exponential decay\n",
    "def calculate_weight(row, date_column, decay_factor):\n",
    "    days_ago = (pd.to_datetime('now') - row[date_column]).days  # Calculate days ago\n",
    "    weight = np.exp(-decay_factor * days_ago)  # Exponential decay function\n",
    "    return weight\n",
    "\n",
    "# Define a decay factor (you can adjust this value based on the desired decay rate)\n",
    "decay_factor = 0.1  # Adjust as needed\n",
    "\n",
    "# Apply the weight function to create a 'Weight' column\n",
    "final_dataset['WEIGHT'] = final_dataset.apply(calculate_weight, args=('DATE', decay_factor), axis=1)\n",
    "\n",
    "\n",
    "final_dataset.to_csv('datasets/weekly_scoring.csv', index=False)\n",
    "\n",
    "## PRINT DATAFRAME DATA\n",
    "# Assuming you have a DataFrame named 'df'\n",
    "# Get the number of rows\n",
    "num_rows = final_dataset.shape[0]\n",
    "\n",
    "# Get the number of columns\n",
    "num_columns = final_dataset.shape[1]\n",
    "\n",
    "# Calculate the product of rows and columns\n",
    "rows_times_columns = num_rows * num_columns\n",
    "\n",
    "# Print the results\n",
    "print(f\"Number of Rows: {num_rows}\")\n",
    "print(f\"Number of Columns: {num_columns}\")\n",
    "print(f\"Rows times Columns: {rows_times_columns}\")\n",
    "\n",
    "file_path = \"datasets/weekly_scoring.csv\"  # Replace with the path to your CSV file\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_path):\n",
    "    # Get the size of the file in bytes\n",
    "    file_size_bytes = os.path.getsize(file_path)\n",
    "\n",
    "    # Convert bytes to megabytes\n",
    "    file_size_mb = file_size_bytes / (1024 * 1024)\n",
    "\n",
    "    print(f\"File Size: {file_size_mb:.2f} MB\")\n",
    "else:\n",
    "    print(\"File not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push Updated Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is ahead of 'origin/main' by 1 commit.\n",
      "  (use \"git push\" to publish your local commits)\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add <file>...\" to update what will be committed)\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\n",
      "\tmodified:   DataPreprocess.ipynb\n",
      "\tmodified:   requirements.txt\n",
      "\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n",
      "File datasets/ successfully pushed to the repository.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Everything up-to-date\n"
     ]
    }
   ],
   "source": [
    "# import subprocess\n",
    "\n",
    "# # List of file paths to push\n",
    "# file_paths = [\"datasets/\"]\n",
    "\n",
    "# # Specify the GitHub repository URL\n",
    "# repo_url = \"https://github.com/jtaylor515/FFanalysis.git\"\n",
    "\n",
    "# # Specify your commit message\n",
    "# commit_message = \"Update dataset files\"\n",
    "\n",
    "# # Git commands to add, commit, and push each file in the list\n",
    "# for file_path in file_paths:\n",
    "#     try:\n",
    "#         subprocess.run([\"git\", \"add\", file_path])\n",
    "#         subprocess.run([\"git\", \"commit\", \"-m\", commit_message])\n",
    "#         subprocess.run([\"git\", \"push\", repo_url])\n",
    "#         print(f\"File {file_path} successfully pushed to the repository.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
