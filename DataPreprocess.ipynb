{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeffreytaylor/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nbformat\n",
    "from io import StringIO\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape basic stat datasets from FantasyPros.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape overall scoring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of URLs to scrape for fantasy stats\n",
    "urls = [\n",
    "    'https://www.fantasypros.com/nfl/stats/qb.php?scoring=HALF&roster=y',\n",
    "    'https://www.fantasypros.com/nfl/stats/rb.php?scoring=HALF&roster=y',\n",
    "    'https://www.fantasypros.com/nfl/stats/wr.php?scoring=HALF&roster=y',\n",
    "    'https://www.fantasypros.com/nfl/stats/te.php?scoring=HALF&roster=y'\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "data_frames = []\n",
    "\n",
    "for url in urls:\n",
    "    # Send an HTTP GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find the table on the page\n",
    "        table = soup.find('table')\n",
    "\n",
    "        table_string = str(table)\n",
    "        table_io = StringIO(table_string)\n",
    "        # Read the table into a DataFrame\n",
    "        df = pd.read_html(table_io)[0]       \n",
    "        \n",
    "        # Add a \"LOC\" column to the DataFrame\n",
    "        loc = url.split('/')[-1][:2]\n",
    "        df[(\"LOC\", \"POS\")] = loc\n",
    "\n",
    "        data_frames.append(df)\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data from {url}\")\n",
    "\n",
    "# Merge all DataFrames into one based on the first and second row headers\n",
    "merged_df = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "# Combine values in column names (headers) and row 0\n",
    "merged_df.columns = merged_df.columns.map(' '.join)\n",
    "\n",
    "# Reset the index\n",
    "merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Rename columns as specified\n",
    "merged_df = merged_df.rename(columns={\"Unnamed: 0_level_0 Rank\": \"POS RANK\", \"Unnamed: 1_level_0 Player\": \"PLAYER\", \"LOC POS\": \"POS\"})\n",
    "\n",
    "# # Get the current working directory\n",
    "# current_directory = os.getcwd()\n",
    "\n",
    "# # Get the parent directory\n",
    "# parent_directory = os.path.dirname(current_directory)\n",
    "\n",
    "# # Change the working directory to the parent directory\n",
    "# os.chdir(parent_directory)\n",
    "\n",
    "merged_df.to_csv('datasets/overall_scoring.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape snap counts data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# List of URLs to scrape for snap counts\n",
    "snap_count_urls = [\n",
    "    'https://www.fantasypros.com/nfl/reports/snap-counts/rb.php?show=perc',\n",
    "    'https://www.fantasypros.com/nfl/reports/snap-counts/wr.php?show=perc',\n",
    "    'https://www.fantasypros.com/nfl/reports/snap-counts/te.php?show=perc'\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store DataFrames for snap counts\n",
    "snap_count_data_frames = []\n",
    "\n",
    "for url in snap_count_urls:\n",
    "    # Send an HTTP GET request to the URL for snap counts\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find the table on the page\n",
    "        table = soup.find('table')\n",
    "\n",
    "        html_content = str(table)\n",
    "        df = pd.read_html(StringIO(html_content), header=[0])[0]  \n",
    "        \n",
    "        # Add a \"POS\" column to the DataFrame for snap counts\n",
    "        pos = url.split('/')[-1][:2]\n",
    "        df[(\"POS\")] = pos\n",
    "\n",
    "        snap_count_data_frames.append(df)\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data from {url} (snap counts)\")\n",
    "\n",
    "# Concatenate (append) all DataFrames for snap counts\n",
    "snap_count_merged_df = pd.concat(snap_count_data_frames, ignore_index=True)\n",
    "\n",
    "# If you want to save the data to a CSV file, you can do it like this:\n",
    "snap_count_merged_df.to_csv('datasets/snap_counts.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape weekly historical scoring data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate URLs to scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get user input for the number of weeks to scrape\n",
    "num_weeks = int(input(\"Enter the current week: \"))\n",
    "\n",
    "# Define the base URL and the page options\n",
    "base_url = \"https://www.fantasypros.com/nfl/stats/\"\n",
    "\n",
    "# List of page options\n",
    "pages = ['qb.php', 'wr.php', 'rb.php', 'te.php']\n",
    "\n",
    "# Initialize the list to store the generated URLs\n",
    "urls = []\n",
    "\n",
    "# Generate URLs based on user input for the 2020, 2021, 2022, and 2023 seasons\n",
    "for page in pages:\n",
    "    for year in range(2018, 2024):  # Loop through years 2020, 2021, 2022, and 2023\n",
    "        max_week = 18 if year > 2020 else 17  # Weeks 1-17 for 2020, Weeks 1-18 for 2021 and 2022\n",
    "        for week in range(1, max_week + 1):\n",
    "            if year == 2023 and week > num_weeks: # Weeks 1-INPUT for 2023\n",
    "                break  # Stop generating URLs for 2022 if the desired week is reached\n",
    "            url = f\"{base_url}{page}?year={year}&range=week&week={week}\"\n",
    "            urls.append(url)\n",
    "\n",
    "# Print the list of generated URLs\n",
    "# for url in urls:\n",
    "#     print(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows: 37915\n",
      "Number of Columns: 30\n",
      "Rows times Columns: 1137450\n",
      "File Size: 3.39 MB\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty DataFrame to store the data\n",
    "final_dataset = pd.DataFrame()\n",
    "\n",
    "# Iterate through the URLs\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Assuming the data is in a table, you may need to adjust the code based on the actual structure\n",
    "        table = soup.find('table')\n",
    "\n",
    "    \n",
    "        table_string = str(table)\n",
    "        table_io = StringIO(table_string)\n",
    "        # Read the table into a DataFrame\n",
    "        df = pd.read_html(table_io)[0]\n",
    "\n",
    "        # Add a \"LOC\" column to the DataFrame\n",
    "        loc = url.split('/')[-1][:2]\n",
    "        df[(\"LOC\", \"POS\")] = loc\n",
    "        \n",
    "        # Extract week value from the URL\n",
    "        week_value = int(url.split('week=')[1])\n",
    "        \n",
    "        # Add a new 'Week' column with the week value\n",
    "        df['WEEK'] = week_value\n",
    "        \n",
    "        # Concatenate the DataFrame to the final dataset\n",
    "        final_dataset = pd.concat([final_dataset, df], ignore_index=True)\n",
    "    else:\n",
    "        print(f\"Failed to fetch data from URL: {url}\")\n",
    "\n",
    "# Now, final_dataset contains the combined data with a 'Week' column\n",
    "final_dataset.head(10)\n",
    "\n",
    "# Combine values in column names (headers) and row 0\n",
    "final_dataset.columns = final_dataset.columns.map(' '.join)\n",
    "\n",
    "# Reset the index\n",
    "final_dataset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Rename columns as specified\n",
    "final_dataset = final_dataset.rename(columns={\"Unnamed: 0_level_0 Rank\": \"POS RANK\", \"Unnamed: 1_level_0 Player\": \"PLAYER\", \"LOC POS\": \"POS\", \"WEEK \": \"WEEK\"})\n",
    "\n",
    "final_dataset.to_csv('datasets/weekly_scoring.csv', index=False)\n",
    "\n",
    "\n",
    "## PRINT DATAFRAME DATA\n",
    "# Assuming you have a DataFrame named 'df'\n",
    "# Get the number of rows\n",
    "num_rows = final_dataset.shape[0]\n",
    "\n",
    "# Get the number of columns\n",
    "num_columns = final_dataset.shape[1]\n",
    "\n",
    "# Calculate the product of rows and columns\n",
    "rows_times_columns = num_rows * num_columns\n",
    "\n",
    "# Print the results\n",
    "print(f\"Number of Rows: {num_rows}\")\n",
    "print(f\"Number of Columns: {num_columns}\")\n",
    "print(f\"Rows times Columns: {rows_times_columns}\")\n",
    "\n",
    "file_path = \"datasets/weekly_scoring.csv\"  # Replace with the path to your CSV file\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_path):\n",
    "    # Get the size of the file in bytes\n",
    "    file_size_bytes = os.path.getsize(file_path)\n",
    "\n",
    "    # Convert bytes to megabytes\n",
    "    file_size_mb = file_size_bytes / (1024 * 1024)\n",
    "\n",
    "    print(f\"File Size: {file_size_mb:.2f} MB\")\n",
    "else:\n",
    "    print(\"File not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push Updated Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 513e395] Update dataset files\n",
      " 2 files changed, 39135 insertions(+), 38736 deletions(-)\n",
      "File datasets/ successfully pushed to the repository.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://github.com/jtaylor515/FFanalysis.git\n",
      "   aeff4be..513e395  main -> main\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# List of file paths to push\n",
    "file_paths = [\"datasets/\"]\n",
    "\n",
    "# Specify the GitHub repository URL\n",
    "repo_url = \"https://github.com/jtaylor515/FFanalysis.git\"\n",
    "\n",
    "# Specify your commit message\n",
    "commit_message = \"Update dataset files\"\n",
    "\n",
    "# Git commands to add, commit, and push each file in the list\n",
    "for file_path in file_paths:\n",
    "    try:\n",
    "        subprocess.run([\"git\", \"add\", file_path])\n",
    "        subprocess.run([\"git\", \"commit\", \"-m\", commit_message])\n",
    "        subprocess.run([\"git\", \"push\", repo_url])\n",
    "        print(f\"File {file_path} successfully pushed to the repository.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
