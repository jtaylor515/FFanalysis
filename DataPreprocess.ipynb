{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nbformat\n",
    "from io import StringIO\n",
    "import os\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape basic stat datasets from FantasyPros.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape overall scoring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of URLs to scrape for fantasy stats\n",
    "urls = [\n",
    "    'https://www.fantasypros.com/nfl/stats/qb.php?scoring=HALF&roster=y',\n",
    "    'https://www.fantasypros.com/nfl/stats/rb.php?scoring=HALF&roster=y',\n",
    "    'https://www.fantasypros.com/nfl/stats/wr.php?scoring=HALF&roster=y',\n",
    "    'https://www.fantasypros.com/nfl/stats/te.php?scoring=HALF&roster=y'\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "data_frames = []\n",
    "\n",
    "for url in urls:\n",
    "    # Send an HTTP GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find the table on the page\n",
    "        table = soup.find('table')\n",
    "\n",
    "        table_string = str(table)\n",
    "        table_io = StringIO(table_string)\n",
    "        # Read the table into a DataFrame\n",
    "        df = pd.read_html(table_io)[0]       \n",
    "        \n",
    "        # Add a \"LOC\" column to the DataFrame\n",
    "        loc = url.split('/')[-1][:2]\n",
    "        df[(\"LOC\", \"POS\")] = loc\n",
    "\n",
    "        data_frames.append(df)\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data from {url}\")\n",
    "\n",
    "# Merge all DataFrames into one based on the first and second row headers\n",
    "merged_df = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "# Combine values in column names (headers) and row 0\n",
    "merged_df.columns = merged_df.columns.map(' '.join)\n",
    "\n",
    "# Reset the index\n",
    "merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Rename columns as specified\n",
    "merged_df = merged_df.rename(columns={\"Unnamed: 0_level_0 Rank\": \"POS RANK\", \"Unnamed: 1_level_0 Player\": \"PLAYER\", \"LOC POS\": \"POS\"})\n",
    "\n",
    "# # Get the current working directory\n",
    "# current_directory = os.getcwd()\n",
    "\n",
    "# # Get the parent directory\n",
    "# parent_directory = os.path.dirname(current_directory)\n",
    "\n",
    "# # Change the working directory to the parent directory\n",
    "# os.chdir(parent_directory)\n",
    "\n",
    "merged_df.to_csv('datasets/overall_scoring.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape snap counts data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# List of URLs to scrape for snap counts\n",
    "snap_count_urls = [\n",
    "    'https://www.fantasypros.com/nfl/reports/snap-counts/rb.php?show=perc',\n",
    "    'https://www.fantasypros.com/nfl/reports/snap-counts/wr.php?show=perc',\n",
    "    'https://www.fantasypros.com/nfl/reports/snap-counts/te.php?show=perc'\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store DataFrames for snap counts\n",
    "snap_count_data_frames = []\n",
    "\n",
    "for url in snap_count_urls:\n",
    "    # Send an HTTP GET request to the URL for snap counts\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find the table on the page\n",
    "        table = soup.find('table')\n",
    "\n",
    "        html_content = str(table)\n",
    "        df = pd.read_html(StringIO(html_content), header=[0])[0]  \n",
    "        \n",
    "        # Add a \"POS\" column to the DataFrame for snap counts\n",
    "        pos = url.split('/')[-1][:2]\n",
    "        df[(\"POS\")] = pos\n",
    "\n",
    "        snap_count_data_frames.append(df)\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data from {url} (snap counts)\")\n",
    "\n",
    "# Concatenate (append) all DataFrames for snap counts\n",
    "snap_count_merged_df = pd.concat(snap_count_data_frames, ignore_index=True)\n",
    "\n",
    "# If you want to save the data to a CSV file, you can do it like this:\n",
    "snap_count_merged_df.to_csv('datasets/snap_counts.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape weekly historical scoring data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate URLs to scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get user input for the number of weeks to scrape\n",
    "num_weeks = int(input(\"Enter the previous week for data collection: \"))\n",
    "\n",
    "# Define the base URL and the page options\n",
    "base_url = \"https://www.fantasypros.com/nfl/stats/\"\n",
    "\n",
    "# List of page options\n",
    "pages = ['qb.php', 'wr.php', 'rb.php', 'te.php']\n",
    "\n",
    "# Initialize the list to store the generated URLs\n",
    "urls = []\n",
    "\n",
    "# Generate URLs based on user input for the 2020, 2021, 2022, and 2023 seasons\n",
    "for page in pages:\n",
    "    for year in range(2021, 2024):  # Loop through years 2020, 2021, 2022, and 2023\n",
    "        max_week = 18 if year > 2020 else 17  # Weeks 1-17 for 2020, Weeks 1-18 for 2021 and 2022\n",
    "        for week in range(1, max_week + 1):\n",
    "            if year == 2023 and week > num_weeks: # Weeks 1-INPUT for 2023\n",
    "                break  # Stop generating URLs for 2022 if the desired week is reached\n",
    "            url = f\"{base_url}{page}?year={year}&range=week&week={week}\"\n",
    "            urls.append(url)\n",
    "\n",
    "# # Print the list of generated URLs\n",
    "# for url in urls:\n",
    "#     print(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qd/rhs7cnfj66l4p8v_yrkw8n4m0000gn/T/ipykernel_72881/1075884743.py:69: FutureWarning: The parsing of 'now' in pd.to_datetime without `utc=True` is deprecated. In a future version, this will match Timestamp('now') and Timestamp.now()\n",
      "  days_ago = (pd.to_datetime('now') - row[date_column]).days  # Calculate days ago\n"
     ]
    }
   ],
   "source": [
    "final_dataset = pd.DataFrame()\n",
    "\n",
    "# Iterate through the URLs\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Assuming the data is in a table, you may need to adjust the code based on the actual structure\n",
    "        table = soup.find('table')\n",
    "\n",
    "    \n",
    "        table_string = str(table)\n",
    "        table_io = StringIO(table_string)\n",
    "        # Read the table into a DataFrame\n",
    "        df = pd.read_html(table_io)[0]\n",
    "\n",
    "        # Add a \"LOC\" column to the DataFrame\n",
    "        loc = url.split('/')[-1][:2]\n",
    "        df[(\"LOC\", \"POS\")] = loc\n",
    "        \n",
    "        # Extract week value from the URL\n",
    "        week_value = int(url.split('week=')[1])\n",
    "        # Regular expression pattern to match the year\n",
    "        pattern = r'year=(\\d{4})'\n",
    "\n",
    "        # Use re.search to find the match\n",
    "        match = re.search(pattern, url)\n",
    "        year_value = match.group(1)\n",
    "        \n",
    "        # Add a new 'Week' column with the week value\n",
    "        df['WEEK'] = week_value\n",
    "        df['YEAR'] = int(year_value)\n",
    "        \n",
    "        # Concatenate the DataFrame to the final dataset\n",
    "        final_dataset = pd.concat([final_dataset, df], ignore_index=True)\n",
    "    else:\n",
    "        print(f\"Failed to fetch data from URL: {url}\")\n",
    "\n",
    "# Now, final_dataset contains the combined data with a 'Week' column\n",
    "final_dataset.head(10)\n",
    "\n",
    "# Combine values in column names (headers) and row 0\n",
    "final_dataset.columns = final_dataset.columns.map(' '.join)\n",
    "\n",
    "# Reset the index\n",
    "final_dataset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Rename columns as specified\n",
    "final_dataset = final_dataset.rename(columns={\"Unnamed: 0_level_0 Rank\": \"POS RANK\", \"Unnamed: 1_level_0 Player\": \"PLAYER\", \"LOC POS\": \"POS\", \"WEEK \": \"WEEK\", \"YEAR \": \"YEAR\"})\n",
    "\n",
    "# Create weighted on date column\n",
    "# Define a custom function to create a date\n",
    "def create_date(row):\n",
    "    year = row['YEAR']\n",
    "    week = row['WEEK']\n",
    "    day_of_week = 1  # You can choose a specific day of the week\n",
    "\n",
    "    # Create a date by considering the year, week, and day_of_week\n",
    "    date = pd.to_datetime(f'{year}-W{week}-{day_of_week}', format='%Y-W%U-%w')\n",
    "\n",
    "    return date\n",
    "\n",
    "# Apply the custom function to create the 'Date' column\n",
    "final_dataset['DATE'] = final_dataset.apply(create_date, axis=1)\n",
    "\n",
    "# Define your custom weight function, for example, exponential decay\n",
    "def calculate_weight(row, date_column, decay_factor):\n",
    "    days_ago = (pd.to_datetime('now') - row[date_column]).days  # Calculate days ago\n",
    "    weight = np.exp(-decay_factor * days_ago)  # Exponential decay function\n",
    "    return weight\n",
    "\n",
    "# Define a decay factor (you can adjust this value based on the desired decay rate)\n",
    "decay_factor = 0.1  # Adjust as needed\n",
    "\n",
    "# Apply the weight function to create a 'Weight' column\n",
    "final_dataset['WEIGHT'] = final_dataset.apply(calculate_weight, args=('DATE', decay_factor), axis=1)\n",
    "\n",
    "pattern = r'\\((.*?)\\)'\n",
    "final_dataset['TEAM'] = final_dataset['PLAYER'].apply(lambda x: re.search(pattern, x).group(1) if re.search(pattern, x) else pd.NA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year 2021 data appended to datasets/master_schedule.csv\n",
      "========================================\n",
      "\n",
      "Year 2022 data appended to datasets/master_schedule.csv\n",
      "========================================\n",
      "\n",
      "Year 2023 data appended to datasets/master_schedule.csv\n",
      "========================================\n",
      "\n",
      "All data combined and saved to datasets/master_schedule.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>TEAM</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021</td>\n",
       "      <td>ARI</td>\n",
       "      <td>TEN</td>\n",
       "      <td>MIN</td>\n",
       "      <td>JAX</td>\n",
       "      <td>LAR</td>\n",
       "      <td>SF</td>\n",
       "      <td>CLE</td>\n",
       "      <td>HOU</td>\n",
       "      <td>GB</td>\n",
       "      <td>SF</td>\n",
       "      <td>CAR</td>\n",
       "      <td>SEA</td>\n",
       "      <td>BYE</td>\n",
       "      <td>CHI</td>\n",
       "      <td>LAR</td>\n",
       "      <td>DET</td>\n",
       "      <td>IND</td>\n",
       "      <td>DAL</td>\n",
       "      <td>SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021</td>\n",
       "      <td>ATL</td>\n",
       "      <td>PHI</td>\n",
       "      <td>TB</td>\n",
       "      <td>NYG</td>\n",
       "      <td>WSH</td>\n",
       "      <td>NYJ</td>\n",
       "      <td>BYE</td>\n",
       "      <td>MIA</td>\n",
       "      <td>CAR</td>\n",
       "      <td>NO</td>\n",
       "      <td>DAL</td>\n",
       "      <td>NE</td>\n",
       "      <td>JAX</td>\n",
       "      <td>TB</td>\n",
       "      <td>CAR</td>\n",
       "      <td>SF</td>\n",
       "      <td>DET</td>\n",
       "      <td>BUF</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021</td>\n",
       "      <td>BAL</td>\n",
       "      <td>LV</td>\n",
       "      <td>KC</td>\n",
       "      <td>DET</td>\n",
       "      <td>DEN</td>\n",
       "      <td>IND</td>\n",
       "      <td>LAC</td>\n",
       "      <td>CIN</td>\n",
       "      <td>BYE</td>\n",
       "      <td>MIN</td>\n",
       "      <td>MIA</td>\n",
       "      <td>CHI</td>\n",
       "      <td>CLE</td>\n",
       "      <td>PIT</td>\n",
       "      <td>CLE</td>\n",
       "      <td>GB</td>\n",
       "      <td>CIN</td>\n",
       "      <td>LAR</td>\n",
       "      <td>PIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021</td>\n",
       "      <td>BUF</td>\n",
       "      <td>PIT</td>\n",
       "      <td>MIA</td>\n",
       "      <td>WSH</td>\n",
       "      <td>HOU</td>\n",
       "      <td>KC</td>\n",
       "      <td>TEN</td>\n",
       "      <td>BYE</td>\n",
       "      <td>MIA</td>\n",
       "      <td>JAX</td>\n",
       "      <td>NYJ</td>\n",
       "      <td>IND</td>\n",
       "      <td>NO</td>\n",
       "      <td>NE</td>\n",
       "      <td>TB</td>\n",
       "      <td>CAR</td>\n",
       "      <td>NE</td>\n",
       "      <td>ATL</td>\n",
       "      <td>NYJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021</td>\n",
       "      <td>CAR</td>\n",
       "      <td>NYJ</td>\n",
       "      <td>NO</td>\n",
       "      <td>HOU</td>\n",
       "      <td>DAL</td>\n",
       "      <td>PHI</td>\n",
       "      <td>MIN</td>\n",
       "      <td>NYG</td>\n",
       "      <td>ATL</td>\n",
       "      <td>NE</td>\n",
       "      <td>ARI</td>\n",
       "      <td>WSH</td>\n",
       "      <td>MIA</td>\n",
       "      <td>BYE</td>\n",
       "      <td>ATL</td>\n",
       "      <td>BUF</td>\n",
       "      <td>TB</td>\n",
       "      <td>NO</td>\n",
       "      <td>TB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021</td>\n",
       "      <td>CHI</td>\n",
       "      <td>LAR</td>\n",
       "      <td>CIN</td>\n",
       "      <td>CLE</td>\n",
       "      <td>DET</td>\n",
       "      <td>LV</td>\n",
       "      <td>GB</td>\n",
       "      <td>TB</td>\n",
       "      <td>SF</td>\n",
       "      <td>PIT</td>\n",
       "      <td>BYE</td>\n",
       "      <td>BAL</td>\n",
       "      <td>DET</td>\n",
       "      <td>ARI</td>\n",
       "      <td>GB</td>\n",
       "      <td>MIN</td>\n",
       "      <td>SEA</td>\n",
       "      <td>NYG</td>\n",
       "      <td>MIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021</td>\n",
       "      <td>CIN</td>\n",
       "      <td>MIN</td>\n",
       "      <td>CHI</td>\n",
       "      <td>PIT</td>\n",
       "      <td>JAX</td>\n",
       "      <td>GB</td>\n",
       "      <td>DET</td>\n",
       "      <td>BAL</td>\n",
       "      <td>NYJ</td>\n",
       "      <td>CLE</td>\n",
       "      <td>BYE</td>\n",
       "      <td>LV</td>\n",
       "      <td>PIT</td>\n",
       "      <td>LAC</td>\n",
       "      <td>SF</td>\n",
       "      <td>DEN</td>\n",
       "      <td>BAL</td>\n",
       "      <td>KC</td>\n",
       "      <td>CLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021</td>\n",
       "      <td>CLE</td>\n",
       "      <td>KC</td>\n",
       "      <td>HOU</td>\n",
       "      <td>CHI</td>\n",
       "      <td>MIN</td>\n",
       "      <td>LAC</td>\n",
       "      <td>ARI</td>\n",
       "      <td>DEN</td>\n",
       "      <td>PIT</td>\n",
       "      <td>CIN</td>\n",
       "      <td>NE</td>\n",
       "      <td>DET</td>\n",
       "      <td>BAL</td>\n",
       "      <td>BYE</td>\n",
       "      <td>BAL</td>\n",
       "      <td>LV</td>\n",
       "      <td>GB</td>\n",
       "      <td>PIT</td>\n",
       "      <td>CIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021</td>\n",
       "      <td>DAL</td>\n",
       "      <td>TB</td>\n",
       "      <td>LAC</td>\n",
       "      <td>PHI</td>\n",
       "      <td>CAR</td>\n",
       "      <td>NYG</td>\n",
       "      <td>NE</td>\n",
       "      <td>BYE</td>\n",
       "      <td>MIN</td>\n",
       "      <td>DEN</td>\n",
       "      <td>ATL</td>\n",
       "      <td>KC</td>\n",
       "      <td>LV</td>\n",
       "      <td>NO</td>\n",
       "      <td>WSH</td>\n",
       "      <td>NYG</td>\n",
       "      <td>WSH</td>\n",
       "      <td>ARI</td>\n",
       "      <td>PHI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021</td>\n",
       "      <td>DEN</td>\n",
       "      <td>NYG</td>\n",
       "      <td>JAX</td>\n",
       "      <td>NYJ</td>\n",
       "      <td>BAL</td>\n",
       "      <td>PIT</td>\n",
       "      <td>LV</td>\n",
       "      <td>CLE</td>\n",
       "      <td>WSH</td>\n",
       "      <td>DAL</td>\n",
       "      <td>PHI</td>\n",
       "      <td>BYE</td>\n",
       "      <td>LAC</td>\n",
       "      <td>KC</td>\n",
       "      <td>DET</td>\n",
       "      <td>CIN</td>\n",
       "      <td>LV</td>\n",
       "      <td>LAC</td>\n",
       "      <td>KC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   YEAR TEAM    1    2    3    4    5    6    7    8    9   10   11   12   13  \\\n",
       "0  2021  ARI  TEN  MIN  JAX  LAR   SF  CLE  HOU   GB   SF  CAR  SEA  BYE  CHI   \n",
       "1  2021  ATL  PHI   TB  NYG  WSH  NYJ  BYE  MIA  CAR   NO  DAL   NE  JAX   TB   \n",
       "2  2021  BAL   LV   KC  DET  DEN  IND  LAC  CIN  BYE  MIN  MIA  CHI  CLE  PIT   \n",
       "3  2021  BUF  PIT  MIA  WSH  HOU   KC  TEN  BYE  MIA  JAX  NYJ  IND   NO   NE   \n",
       "4  2021  CAR  NYJ   NO  HOU  DAL  PHI  MIN  NYG  ATL   NE  ARI  WSH  MIA  BYE   \n",
       "5  2021  CHI  LAR  CIN  CLE  DET   LV   GB   TB   SF  PIT  BYE  BAL  DET  ARI   \n",
       "6  2021  CIN  MIN  CHI  PIT  JAX   GB  DET  BAL  NYJ  CLE  BYE   LV  PIT  LAC   \n",
       "7  2021  CLE   KC  HOU  CHI  MIN  LAC  ARI  DEN  PIT  CIN   NE  DET  BAL  BYE   \n",
       "8  2021  DAL   TB  LAC  PHI  CAR  NYG   NE  BYE  MIN  DEN  ATL   KC   LV   NO   \n",
       "9  2021  DEN  NYG  JAX  NYJ  BAL  PIT   LV  CLE  WSH  DAL  PHI  BYE  LAC   KC   \n",
       "\n",
       "    14   15   16   17   18  \n",
       "0  LAR  DET  IND  DAL  SEA  \n",
       "1  CAR   SF  DET  BUF   NO  \n",
       "2  CLE   GB  CIN  LAR  PIT  \n",
       "3   TB  CAR   NE  ATL  NYJ  \n",
       "4  ATL  BUF   TB   NO   TB  \n",
       "5   GB  MIN  SEA  NYG  MIN  \n",
       "6   SF  DEN  BAL   KC  CLE  \n",
       "7  BAL   LV   GB  PIT  CIN  \n",
       "8  WSH  NYG  WSH  ARI  PHI  \n",
       "9  DET  CIN   LV  LAC   KC  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# URL for the ESPN NFL schedule grid\n",
    "base_url = \"https://www.espn.com/nfl/schedulegrid/_/year/\"\n",
    "\n",
    "# Years for which you want to scrape data\n",
    "years = [2021, 2022, 2023]\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Directory to save the combined CSV file\n",
    "save_directory = \"datasets\"\n",
    "combined_csv_filename = os.path.join(save_directory, \"master_schedule.csv\")\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Flag to indicate whether to include headers or not\n",
    "include_headers = True\n",
    "\n",
    "# Open the file in 'w' mode to overwrite if it already exists\n",
    "with open(combined_csv_filename, 'w', newline='', encoding='utf-8') as combined_csv_file:\n",
    "    combined_csv_writer = csv.writer(combined_csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "    for year in years:\n",
    "        url = f\"{base_url}{year}\"\n",
    "\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content of the page\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Find the tables on the page\n",
    "            tables = soup.find_all('table')\n",
    "\n",
    "            # Process each table\n",
    "            for index, table in enumerate(tables):\n",
    "                # Extract data from the table as needed\n",
    "                rows = table.find_all('tr')\n",
    "\n",
    "                # Prepare data for writing to the combined CSV file\n",
    "                data_rows = []\n",
    "                for row in rows[1:]:\n",
    "                    data_rows.append([year] + [cell.get_text(strip=True) for cell in row.find_all(['th', 'td'])])\n",
    "\n",
    "                # Include headers only for the first iteration\n",
    "                if include_headers:\n",
    "                    combined_csv_writer.writerow([cell.get_text(strip=True) for cell in rows[0].find_all('th')])\n",
    "                    include_headers = False  # Set to False after writing headers\n",
    "\n",
    "                combined_csv_writer.writerows(data_rows[1:])  # Exclude the first row\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data for the year {year}. Status code: {response.status_code}\")\n",
    "\n",
    "print(f\"All data combined and saved to {combined_csv_filename}\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"datasets/master_schedule.csv\", header=None)\n",
    "\n",
    "# List of new column names\n",
    "new_column_names = ['YEAR', 'TEAM', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18']\n",
    "\n",
    "# Use the rename method\n",
    "df.rename(columns=dict(zip(df.columns, new_column_names)), inplace=True)\n",
    "\n",
    "df = df.replace('@', '', regex=True)\n",
    "\n",
    "df.to_csv(\"datasets/master_schedule.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Join and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows: 20647\n",
      "Number of Columns: 34\n",
      "Rows times Columns: 701998\n",
      "File Size: 2.74 MB\n"
     ]
    }
   ],
   "source": [
    "weekly_scoring = pd.read_csv(\"datasets/weekly_scoring.csv\")\n",
    "master_schedule = pd.read_csv(\"datasets/master_schedule.csv\")\n",
    "\n",
    "dataset_a = weekly_scoring\n",
    "dataset_b = master_schedule\n",
    "\n",
    "# Reshape dataset_b to have columns ['week', 'team', 'opponent']\n",
    "melted_dataset_b = pd.melt(dataset_b, id_vars=['YEAR', 'TEAM'], var_name='WEEK', value_name='OPP')\n",
    "\n",
    "# Convert 'WEEK' column to integer\n",
    "melted_dataset_b['WEEK'] = melted_dataset_b['WEEK'].astype(int)\n",
    "\n",
    "# Merge datasets based on 'week', 'team', and 'year'\n",
    "merged_dataset = pd.merge(dataset_a, melted_dataset_b, on=['WEEK', 'TEAM', 'YEAR'], how='left')\n",
    "\n",
    "# Display the merged DataFrame\n",
    "merged_dataset.to_csv(\"datasets/weekly_scoring.csv\", index=False)\n",
    "\n",
    "## PRINT DATAFRAME DATA\n",
    "# Assuming you have a DataFrame named 'df'\n",
    "# Get the number of rows\n",
    "num_rows = final_dataset.shape[0]\n",
    "\n",
    "# Get the number of columns\n",
    "num_columns = final_dataset.shape[1]\n",
    "\n",
    "# Calculate the product of rows and columns\n",
    "rows_times_columns = num_rows * num_columns\n",
    "\n",
    "# Print the results\n",
    "print(f\"Number of Rows: {num_rows}\")\n",
    "print(f\"Number of Columns: {num_columns}\")\n",
    "print(f\"Rows times Columns: {rows_times_columns}\")\n",
    "\n",
    "file_path = \"datasets/weekly_scoring.csv\"  # Replace with the path to your CSV file\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_path):\n",
    "    # Get the size of the file in bytes\n",
    "    file_size_bytes = os.path.getsize(file_path)\n",
    "\n",
    "    # Convert bytes to megabytes\n",
    "    file_size_mb = file_size_bytes / (1024 * 1024)\n",
    "\n",
    "    print(f\"File Size: {file_size_mb:.2f} MB\")\n",
    "else:\n",
    "    print(\"File not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push Updated Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jeff b8b66a8] Update dataset files\n",
      " 2 files changed, 20745 insertions(+), 39639 deletions(-)\n",
      " create mode 100644 datasets/master_schedule.csv\n",
      "File datasets/ successfully pushed to the repository.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://github.com/jtaylor515/FFanalysis.git\n",
      "   3ca9a75..b8b66a8  jeff -> jeff\n"
     ]
    }
   ],
   "source": [
    "# import subprocess\n",
    "\n",
    "# # List of file paths to push\n",
    "# file_paths = [\"datasets/\"]\n",
    "\n",
    "# # Specify the GitHub repository URL\n",
    "# repo_url = \"https://github.com/jtaylor515/FFanalysis.git\"\n",
    "\n",
    "# # Specify your commit message\n",
    "# commit_message = \"Update dataset files\"\n",
    "\n",
    "# # Git commands to add, commit, and push each file in the list\n",
    "# for file_path in file_paths:\n",
    "#     try:\n",
    "#         subprocess.run([\"git\", \"add\", file_path])\n",
    "#         subprocess.run([\"git\", \"commit\", \"-m\", commit_message])\n",
    "#         subprocess.run([\"git\", \"push\", repo_url])\n",
    "#         print(f\"File {file_path} successfully pushed to the repository.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
