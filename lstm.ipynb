{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position: qb\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "Mean Squared Error qb: 24.062670889562465\n",
      "MAE qb: 3.2765796308701396\n",
      "R-squared qb: 0.7043264494965567\n",
      "Top-5 Accuracy: 1.0\n",
      "11/11 [==============================] - 0s 801us/step\n",
      "Permutation Importances:\n",
      "MISC FPTS: -5.829999843620049\n",
      "Position: rb\n",
      "24/24 [==============================] - 0s 904us/step\n",
      "Mean Squared Error rb: 10.620555361282497\n",
      "MAE rb: 1.7032756601493673\n",
      "R-squared rb: 0.7673567743518677\n",
      "Top-5 Accuracy: 1.0\n",
      "24/24 [==============================] - 0s 828us/step\n",
      "Permutation Importances:\n",
      "MISC FPTS: -4.943767438755726\n",
      "Position: wr\n",
      "33/33 [==============================] - 0s 805us/step\n",
      "Mean Squared Error wr: 6.472467127509639\n",
      "MAE wr: 1.1942477445514228\n",
      "R-squared wr: 0.8018377049682401\n",
      "Top-5 Accuracy: 1.0\n",
      "33/33 [==============================] - 0s 776us/step\n",
      "Permutation Importances:\n",
      "MISC FPTS: -4.448657107513423\n",
      "Position: te\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Mean Squared Error te: 5.945061518748014\n",
      "MAE te: 1.458550640211934\n",
      "R-squared te: 0.688425138083816\n",
      "Top-5 Accuracy: 1.0\n",
      "16/16 [==============================] - 0s 970us/step\n",
      "Permutation Importances:\n",
      "MISC FPTS: -2.7674555067611166\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "positions = [\"qb\", \"rb\", \"wr\", \"te\"]\n",
    "# positions = [\"qb\"]\n",
    "\n",
    "for pos in positions:\n",
    "    print(\"Position: \" + pos)\n",
    "    # Load your dataset\n",
    "    data = pd.read_csv(\"datasets/weekly_scoring.csv\")\n",
    "\n",
    "    # Preprocessing\n",
    "    data = data[data['POS'] == pos]\n",
    "\n",
    "    # Drop Zero values (bye weeks, injuries)\n",
    "    data = data[data['MISC FPTS'] != 0]\n",
    "    weights = data['WEIGHT']  \n",
    "\n",
    "    \n",
    "    # Instead of dropping zero values, substitute them with the mean (including zeroes) for everyone\n",
    "    # column_to_replace = 'MISC FPTS'\n",
    "    # mean_value = data[column_to_replace].mean()\n",
    "    # data[column_to_replace] = data[column_to_replace].replace(0, mean_value)\n",
    "    \n",
    "    # Replace zero values with the mean specific to each player\n",
    "    # player_means = data.groupby('PLAYER', group_keys=True)[column_to_replace].max()\n",
    "    # data[column_to_replace] = data.groupby('PLAYER')[column_to_replace].apply(lambda x: x.replace(0, x.mean()))\n",
    "\n",
    "    if(pos == 'qb'):\n",
    "        # Define the list of variables to predict\n",
    "        # columns_to_predict = ['PASSING CMP', 'PASSING ATT', 'PASSING PCT', 'PASSING YDS', 'PASSING Y/A', 'PASSING TD', 'PASSING INT',\n",
    "        # 'PASSING SACKS', 'RUSHING ATT', 'RUSHING YDS', 'RUSHING TD', 'MISC FL', 'WEEK', 'AVG_FPTS', 'MAX_FPTS', 'MIN_FPTS', 'VAR_FPTS']\n",
    "        columns_to_predict = ['MISC FPTS']\n",
    "        # # Drop Zero values (bye weeks, injuries)\n",
    "        # data = data[data['MISC FPTS'] != 0]\n",
    "        look_back = 15\n",
    "    if(pos == 'rb'):\n",
    "        # columns_to_predict = ['RECEIVING REC', 'RECEIVING TGT', 'RECEIVING YDS', 'RECEIVING Y/R',\n",
    "        # 'RECEIVING TD', 'RUSHING Y/A', 'RUSHING LG',\n",
    "        # 'RUSHING 20+', 'RUSHING ATT', 'RUSHING YDS', 'RUSHING TD', 'MISC FL', 'AVG_FPTS', 'MAX_FPTS', 'VAR_FPTS']\n",
    "        columns_to_predict = ['MISC FPTS']\n",
    "        # # Replace zero values with the mean specific to each player\n",
    "        # player_means = data.groupby('PLAYER', group_keys=True)[column_to_replace].max()\n",
    "        # data[column_to_replace] = data.groupby('PLAYER')[column_to_replace].apply(lambda x: x.replace(0, x.mean()))\n",
    "        look_back = 16\n",
    "    if(pos == 'wr'):\n",
    "        # columns_to_predict = ['RECEIVING REC', 'RECEIVING TGT', 'RECEIVING YDS', 'RECEIVING Y/R',\n",
    "        # 'RECEIVING TD', 'RECEIVING LG', 'RECEIVING 20+',\n",
    "        # 'RUSHING ATT', 'RUSHING YDS', 'RUSHING TD', 'MISC FL', 'MISC FPTS', 'WEEK', 'AVG_FPTS', 'MAX_FPTS', 'MIN_FPTS', 'VAR_FPTS']\n",
    "        columns_to_predict = ['MISC FPTS']\n",
    "        # # Replace zero values with the mean specific to each player\n",
    "        # player_means = data.groupby('PLAYER', group_keys=True)[column_to_replace].max()\n",
    "        # data[column_to_replace] = data.groupby('PLAYER')[column_to_replace].apply(lambda x: x.replace(0, x.mean()))\n",
    "        look_back = 14\n",
    "    if(pos == 'te'):\n",
    "        # columns_to_predict = ['RECEIVING REC', 'RECEIVING TGT', 'RECEIVING YDS', 'RECEIVING Y/R',\n",
    "        # 'RECEIVING TD', 'RECEIVING LG', 'RECEIVING 20+',\n",
    "        # 'RUSHING ATT', 'RUSHING YDS', 'RUSHING TD', 'MISC FL', 'MISC FPTS', 'WEEK', 'AVG_FPTS', 'MAX_FPTS', 'MIN_FPTS', 'VAR_FPTS']\n",
    "        columns_to_predict = ['MISC FPTS']\n",
    "        # # Replace zero values with the mean specific to each player\n",
    "        # player_means = data.groupby('PLAYER', group_keys=True)[column_to_replace].max()\n",
    "        # data[column_to_replace] = data.groupby('PLAYER')[column_to_replace].apply(lambda x: x.replace(0, x.mean()))\n",
    "        look_back = 20\n",
    "\n",
    "\n",
    "    # Sort the data by the date column\n",
    "    date_column = \"CONTINUOUS_DATE\"\n",
    "    data[date_column] = pd.to_datetime(data[date_column])\n",
    "    data = data.sort_values(by=date_column)\n",
    "\n",
    "    # Extract the relevant columns for training\n",
    "    training_data = data[columns_to_predict].values\n",
    "\n",
    "    # Impute missing values for training_data\n",
    "    imputer = SimpleImputer(strategy='mean')  # You can change the strategy as needed\n",
    "    training_data = imputer.fit_transform(training_data)\n",
    "    # training_data[np.isnan(training_data)] = 0\n",
    "\n",
    "    # Normalize the data using Min-Max scaling\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    training_data_scaled = scaler.fit_transform(training_data)\n",
    "\n",
    "    # Define a function to create LSTM datasets\n",
    "    def create_lstm_dataset(dataset, look_back=1):\n",
    "        X, Y = [], []\n",
    "        for i in range(len(dataset) - look_back):\n",
    "            X.append(dataset[i:(i + look_back)])\n",
    "            Y.append(dataset[i + look_back])\n",
    "        return np.array(X), np.array(Y)\n",
    "\n",
    "    # Create the LSTM dataset\n",
    "    X, Y = create_lstm_dataset(training_data_scaled, look_back)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    train_size = int(len(X) * 0.8)\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    Y_train, Y_test = Y[:train_size], Y[train_size:]    \n",
    "    # Reshape the input data for LSTM (samples, time steps, features)\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], look_back, X_train.shape[2]))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], look_back, X_test.shape[2]))\n",
    "\n",
    "    # Build the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(Dense(units=len(columns_to_predict)))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, Y_train, epochs=100, batch_size=32, validation_data=(X_test, Y_test), verbose=0)\n",
    "\n",
    "    # print(\"LSTM Model Summary:\")\n",
    "    # model.summary()\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    # Rescale the predictions to the original scale\n",
    "    predictions_rescaled = scaler.inverse_transform(predictions)\n",
    "    Y_test_rescaled = scaler.inverse_transform(Y_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(Y_test_rescaled, predictions_rescaled)\n",
    "    print(f'Mean Squared Error {pos}: {mse}')\n",
    "\n",
    "    # Root Mean Squared Error (RMSE)\n",
    "    # rmse = np.sqrt(mean_squared_error(Y_test_rescaled, predictions_rescaled))\n",
    "    # print(f\"RMSE {pos}: {rmse}\")\n",
    "\n",
    "    # Mean Absolute Error (MAE)\n",
    "    mae = mean_absolute_error(Y_test_rescaled, predictions_rescaled)\n",
    "    print(f\"MAE {pos}: {mae}\")\n",
    "\n",
    "    # R-squared\n",
    "    r_squared = r2_score(Y_test_rescaled, predictions_rescaled)\n",
    "    print(f\"R-squared {pos}: {r_squared}\")\n",
    "\n",
    "    # Top K Accuracy\n",
    "\n",
    "    def top_k_accuracy_metric(y_true, y_pred, k=5):\n",
    "        # Get the indices of the top-k predicted values\n",
    "        top_k_pred = np.argsort(y_pred)[-k:]\n",
    "        top_k_true = np.argsort(y_true)[-k:]\n",
    "        # print(y_true.shape[2])\n",
    "        # print(y_pred.shape[2])\n",
    "        i = 0\n",
    "        for j in top_k_pred:\n",
    "            if j in top_k_true:\n",
    "                i=i+1\n",
    "        \n",
    "        # Calculate the top-k accuracy\n",
    "        top_k_acc = i/k\n",
    "        return top_k_acc\n",
    "\n",
    "    \n",
    "    k = 5  # Choose the value of k for top-k accuracy\n",
    "    top_k_acc = top_k_accuracy_metric(Y_test_rescaled, predictions_rescaled, 5)\n",
    "    print(f\"Top-{k} Accuracy: {top_k_acc}\")\n",
    "\n",
    "   \n",
    "    ############ feature importance ######################\n",
    "\n",
    "    # Create a copy of the original X_test\n",
    "    X_test_permuted = X_test.copy()\n",
    "\n",
    "    # Get the number of features\n",
    "    num_features = X_test.shape[2]\n",
    "\n",
    "    # Initialize an array to store permutation importances\n",
    "    importances = np.zeros(num_features)\n",
    "\n",
    "    # Loop over each feature\n",
    "    for feature_idx in range(num_features):\n",
    "        # Permute the values of the current feature\n",
    "        X_test_permuted[:, :, feature_idx] = np.random.permutation(X_test[:, :, feature_idx])\n",
    "\n",
    "        # Make predictions on the permuted data\n",
    "        predictions_permuted = model.predict(X_test_permuted)\n",
    "\n",
    "        # Rescale the predictions to the original scale\n",
    "        predictions_permuted_rescaled = scaler.inverse_transform(predictions_permuted)\n",
    "\n",
    "        # Calculate the metric of interest (MAE in this case) on the permuted predictions\n",
    "        mae_permuted = mean_absolute_error(Y_test_rescaled, predictions_permuted_rescaled)\n",
    "\n",
    "        # Calculate the change in metric and store the importance\n",
    "        importances[feature_idx] = mae - mae_permuted\n",
    "\n",
    "        # Reset the permuted feature to its original values\n",
    "        X_test_permuted[:, :, feature_idx] = X_test[:, :, feature_idx]\n",
    "\n",
    "    # Print or store the feature importances\n",
    "    feature_indices = np.argsort(importances)[::-1]\n",
    "    print(\"Permutation Importances:\")\n",
    "    for idx in feature_indices:\n",
    "        print(f\"{columns_to_predict[idx]}: {importances[idx]}\")\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    results_list = []\n",
    "\n",
    "    # Create a DataFrame with 'PLAYER' and predicted values\n",
    "    result_df = pd.DataFrame({'PLAYER': data['PLAYER'].iloc[train_size + look_back:], 'Predicted_FPTS': predictions_rescaled[:, 0]})\n",
    "\n",
    "    # Group by 'PLAYER' and calculate the average predicted FPTS\n",
    "    result_df = result_df.groupby('PLAYER').mean().reset_index()\n",
    "    result_df = result_df.sort_values(by='Predicted_FPTS', ascending=False)\n",
    "\n",
    "    # Add the 'TEAM' column if needed (replace with your own logic)\n",
    "    pattern = r'\\((.*?)\\)'\n",
    "    result_df['TEAM'] = result_df['PLAYER'].apply(lambda x: re.search(pattern, x).group(1) if re.search(pattern, x) else pd.NA)\n",
    "\n",
    "    # # Keep only the first unique occurrence of any value in the 'TEAM' column\n",
    "    # result_df = result_df.drop_duplicates(subset='TEAM')\n",
    "\n",
    "    # Append the results to the list\n",
    "    results_list.append(result_df)\n",
    "\n",
    "    # Combine the results from all folds\n",
    "    final_results_df = pd.concat(results_list, ignore_index=True)\n",
    "\n",
    "    # Remove any player with the team equal to 'FA'\n",
    "    final_results_df = final_results_df.query(\"TEAM != 'FA'\")\n",
    "\n",
    "    # Save the results to a CSV file\n",
    "    file_name = f\"predictions/LSTM_predictions_{pos}.csv\"\n",
    "    final_results_df.to_csv(file_name, index=False)\n",
    "\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
