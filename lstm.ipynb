{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Model Summary:\n",
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_22 (LSTM)              (None, 50)                13400     \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 16)                816       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14216 (55.53 KB)\n",
      "Trainable params: 14216 (55.53 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "38/38 [==============================] - 0s 506us/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/jeffreytaylor/Desktop/FFproject/FFanalysis/lstm.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jeffreytaylor/Desktop/FFproject/FFanalysis/lstm.ipynb#W6sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m Y_test_rescaled \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39minverse_transform(Y_test)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jeffreytaylor/Desktop/FFproject/FFanalysis/lstm.ipynb#W6sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m \u001b[39m# # Print Predictions as DataFrame\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jeffreytaylor/Desktop/FFproject/FFanalysis/lstm.ipynb#W6sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m \u001b[39m# predictions_df = pd.DataFrame({\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jeffreytaylor/Desktop/FFproject/FFanalysis/lstm.ipynb#W6sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m \u001b[39m#     'Actual_FPTS': Y_test_rescaled.flatten(),\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jeffreytaylor/Desktop/FFproject/FFanalysis/lstm.ipynb#W6sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m \n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jeffreytaylor/Desktop/FFproject/FFanalysis/lstm.ipynb#W6sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m \u001b[39m# Evaluate the model\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/jeffreytaylor/Desktop/FFproject/FFanalysis/lstm.ipynb#W6sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m mse \u001b[39m=\u001b[39m mean_squared_error(Y_test_rescaled, predictions_rescaled)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jeffreytaylor/Desktop/FFproject/FFanalysis/lstm.ipynb#W6sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mMean Squared Error \u001b[39m\u001b[39m{\u001b[39;00mpos\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mmse\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jeffreytaylor/Desktop/FFproject/FFanalysis/lstm.ipynb#W6sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m \u001b[39m# # Root Mean Squared Error (RMSE)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jeffreytaylor/Desktop/FFproject/FFanalysis/lstm.ipynb#W6sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m \u001b[39m# rmse = np.sqrt(mean_squared_error(Y_test_rescaled, predictions_rescaled))\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jeffreytaylor/Desktop/FFproject/FFanalysis/lstm.ipynb#W6sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m \u001b[39m# print(f\"RMSE {pos}: {rmse}\")\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jeffreytaylor/Desktop/FFproject/FFanalysis/lstm.ipynb#W6sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m \n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jeffreytaylor/Desktop/FFproject/FFanalysis/lstm.ipynb#W6sZmlsZQ%3D%3D?line=121'>122</a>\u001b[0m \u001b[39m# Mean Absolute Error (MAE)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py:474\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[39m@validate_params\u001b[39m(\n\u001b[1;32m    405\u001b[0m     {\n\u001b[1;32m    406\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39my_true\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m\"\u001b[39m\u001b[39marray-like\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    415\u001b[0m     y_true, y_pred, \u001b[39m*\u001b[39m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, multioutput\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39muniform_average\u001b[39m\u001b[39m\"\u001b[39m, squared\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    416\u001b[0m ):\n\u001b[1;32m    417\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Mean squared error regression loss.\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \n\u001b[1;32m    419\u001b[0m \u001b[39m    Read more in the :ref:`User Guide <mean_squared_error>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[39m    0.825...\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 474\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[39m=\u001b[39m _check_reg_targets(\n\u001b[1;32m    475\u001b[0m         y_true, y_pred, multioutput\n\u001b[1;32m    476\u001b[0m     )\n\u001b[1;32m    477\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    478\u001b[0m     output_errors \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39maverage((y_true \u001b[39m-\u001b[39m y_pred) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, weights\u001b[39m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py:101\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m     99\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[1;32m    100\u001b[0m y_true \u001b[39m=\u001b[39m check_array(y_true, ensure_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m--> 101\u001b[0m y_pred \u001b[39m=\u001b[39m check_array(y_pred, ensure_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m    103\u001b[0m \u001b[39mif\u001b[39;00m y_true\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    104\u001b[0m     y_true \u001b[39m=\u001b[39m y_true\u001b[39m.\u001b[39mreshape((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:959\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    953\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    954\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    955\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    956\u001b[0m         )\n\u001b[1;32m    958\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 959\u001b[0m         _assert_all_finite(\n\u001b[1;32m    960\u001b[0m             array,\n\u001b[1;32m    961\u001b[0m             input_name\u001b[39m=\u001b[39minput_name,\n\u001b[1;32m    962\u001b[0m             estimator_name\u001b[39m=\u001b[39mestimator_name,\n\u001b[1;32m    963\u001b[0m             allow_nan\u001b[39m=\u001b[39mforce_all_finite \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mallow-nan\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    964\u001b[0m         )\n\u001b[1;32m    966\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    967\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:124\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    122\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m _assert_all_finite_element_wise(\n\u001b[1;32m    125\u001b[0m     X,\n\u001b[1;32m    126\u001b[0m     xp\u001b[39m=\u001b[39mxp,\n\u001b[1;32m    127\u001b[0m     allow_nan\u001b[39m=\u001b[39mallow_nan,\n\u001b[1;32m    128\u001b[0m     msg_dtype\u001b[39m=\u001b[39mmsg_dtype,\n\u001b[1;32m    129\u001b[0m     estimator_name\u001b[39m=\u001b[39mestimator_name,\n\u001b[1;32m    130\u001b[0m     input_name\u001b[39m=\u001b[39minput_name,\n\u001b[1;32m    131\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:173\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39mif\u001b[39;00m estimator_name \u001b[39mand\u001b[39;00m input_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    157\u001b[0m     \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m    160\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    172\u001b[0m     )\n\u001b[0;32m--> 173\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "import re\n",
    "from matplotlib.dates import DateFormatter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# positions = [\"qb\", \"rb\", \"wr\", \"te\"]\n",
    "positions = [\"rb\"]\n",
    "\n",
    "\n",
    "for pos in positions:\n",
    "    # Load your dataset\n",
    "    data = pd.read_csv(\"datasets/weekly_scoring.csv\")\n",
    "\n",
    "    # Preprocessing\n",
    "    data = data[data['POS'] == pos]\n",
    "    # data = data[data['PLAYER'] == 'Christian McCaffrey (SF)']\n",
    "\n",
    "    weights = data['WEIGHT']\n",
    "    \n",
    "    if(pos == 'qb'):\n",
    "        # Define the list of variables to predict\n",
    "        columns_to_predict = ['PASSING CMP', 'PASSING ATT', 'PASSING PCT', 'PASSING YDS', 'PASSING Y/A', 'PASSING TD', 'PASSING INT',\n",
    "        'PASSING SACKS', 'RUSHING ATT', 'RUSHING YDS', 'RUSHING TD', 'MISC FL', 'WEEK', 'AVG_FPTS', 'MAX_FPTS', 'MIN_FPTS', 'VAR_FPTS']\n",
    "    if(pos == 'rb'):\n",
    "        columns_to_predict = ['RECEIVING REC', 'RECEIVING TGT', 'RECEIVING YDS', 'RECEIVING Y/R',\n",
    "        'RECEIVING TD', 'RUSHING Y/A', 'RUSHING LG',\n",
    "        'RUSHING 20+', 'RUSHING ATT', 'RUSHING YDS', 'RUSHING TD', 'MISC FL', 'MISC FPTS', 'AVG_FPTS', 'MAX_FPTS', 'VAR_FPTS']\n",
    "    if(pos == 'wr'):\n",
    "        columns_to_predict = ['RECEIVING REC', 'RECEIVING TGT', 'RECEIVING YDS', 'RECEIVING Y/R',\n",
    "        'RECEIVING TD', 'RECEIVING LG', 'RECEIVING 20+',\n",
    "        'RUSHING ATT', 'RUSHING YDS', 'RUSHING TD', 'MISC FL', 'MISC FPTS', 'WEEK', 'AVG_FPTS', 'MAX_FPTS', 'MIN_FPTS', 'VAR_FPTS']\n",
    "    if(pos == 'te'):\n",
    "        columns_to_predict = ['RECEIVING REC', 'RECEIVING TGT', 'RECEIVING YDS', 'RECEIVING Y/R',\n",
    "        'RECEIVING TD', 'RECEIVING LG', 'RECEIVING 20+',\n",
    "        'RUSHING ATT', 'RUSHING YDS', 'RUSHING TD', 'MISC FL', 'MISC FPTS', 'WEEK', 'AVG_FPTS', 'MAX_FPTS', 'MIN_FPTS', 'VAR_FPTS']\n",
    "\n",
    "    # Sort the data by the date column\n",
    "    date_column = \"CONTINUOUS_DATE\"\n",
    "    data[date_column] = pd.to_datetime(data[date_column])\n",
    "    data = data.sort_values(by=date_column)\n",
    "\n",
    "    # Extract the relevant columns for training\n",
    "    training_data = data[columns_to_predict].values\n",
    "\n",
    "    # Impute missing values for training_data\n",
    "    # Handling missing values in numeric columns using SimpleImputer\n",
    "    imputer = SimpleImputer(strategy='mean')  # You can change the strategy as needed\n",
    "    # training_data = imputer.fit_transform(training_data)\n",
    "    # # training_data[np.isnan(training_data)] = 0\n",
    "    ## Drop Zero values (bye weeks, injuries)\n",
    "    data = data[data['MISC FPTS'] != 0]\n",
    "\n",
    "    # Normalize the data using Min-Max scaling\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    training_data_scaled = scaler.fit_transform(training_data)\n",
    "\n",
    "    # Define a function to create LSTM datasets\n",
    "    def create_lstm_dataset(dataset, look_back=1):\n",
    "        X, Y = [], []\n",
    "        for i in range(len(dataset) - look_back):\n",
    "            X.append(dataset[i:(i + look_back)])\n",
    "            Y.append(dataset[i + look_back])\n",
    "        return np.array(X), np.array(Y)\n",
    "\n",
    "    # Set the number of time steps to look back\n",
    "    look_back = 5  # You can adjust this value based on the characteristics of your data\n",
    "\n",
    "    # Create the LSTM dataset\n",
    "    X, Y = create_lstm_dataset(training_data_scaled, look_back)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    train_size = int(len(X) * 0.8)\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    Y_train, Y_test = Y[:train_size], Y[train_size:]\n",
    "\n",
    "    # Reshape the input data for LSTM (samples, time steps, features)\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], look_back, X_train.shape[2]))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], look_back, X_test.shape[2]))\n",
    "\n",
    "    # Build the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(Dense(units=len(columns_to_predict)))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    print(\"LSTM Model Summary:\")\n",
    "    model.summary()\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, Y_train, epochs=100, batch_size=32, validation_data=(X_test, Y_test), verbose=0)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    # Rescale the predictions to the original scale\n",
    "    predictions_rescaled = scaler.inverse_transform(predictions)\n",
    "    Y_test_rescaled = scaler.inverse_transform(Y_test)\n",
    "\n",
    "    # # Print Predictions as DataFrame\n",
    "    # predictions_df = pd.DataFrame({\n",
    "    #     'Actual_FPTS': Y_test_rescaled.flatten(),\n",
    "    #     'Predicted_FPTS': predictions_rescaled.flatten(),\n",
    "    # })\n",
    "    # print(\"\\nPredictions DataFrame:\")\n",
    "    # print(predictions_df)\n",
    "\n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(Y_test_rescaled, predictions_rescaled)\n",
    "    print(f'Mean Squared Error {pos}: {mse}')\n",
    "\n",
    "    # # Root Mean Squared Error (RMSE)\n",
    "    # rmse = np.sqrt(mean_squared_error(Y_test_rescaled, predictions_rescaled))\n",
    "    # print(f\"RMSE {pos}: {rmse}\")\n",
    "\n",
    "    # Mean Absolute Error (MAE)\n",
    "    mae = mean_absolute_error(Y_test_rescaled, predictions_rescaled)\n",
    "    print(f\"MAE {pos}: {mae}\")\n",
    "\n",
    "    # R-squared\n",
    "    r_squared = r2_score(Y_test_rescaled, predictions_rescaled)\n",
    "    print(f\"R-squared {pos}: {r_squared}\")\n",
    "\n",
    "    # # Create a DataFrame to store the results\n",
    "    # results_list = []\n",
    "\n",
    "    # # Create a DataFrame with 'PLAYER' and predicted values\n",
    "    # result_df = pd.DataFrame({'PLAYER': data['PLAYER'].iloc[train_size + look_back:], 'Predicted_FPTS': predictions_rescaled[:, 0]})\n",
    "\n",
    "    # # Group by 'PLAYER' and calculate the average predicted FPTS\n",
    "    # result_df = result_df.groupby('PLAYER').mean().reset_index()\n",
    "    # result_df = result_df.sort_values(by='Predicted_FPTS', ascending=False)\n",
    "\n",
    "    # # Add the 'TEAM' column if needed (replace with your own logic)\n",
    "    # pattern = r'\\((.*?)\\)'\n",
    "    # result_df['TEAM'] = result_df['PLAYER'].apply(lambda x: re.search(pattern, x).group(1) if re.search(pattern, x) else pd.NA)\n",
    "\n",
    "    # # Keep only the first unique occurrence of any value in the 'TEAM' column\n",
    "    # result_df = result_df.drop_duplicates(subset='TEAM')\n",
    "\n",
    "    # # Append the results to the list\n",
    "    # results_list.append(result_df)\n",
    "\n",
    "    # # Combine the results from all folds\n",
    "    # final_results_df = pd.concat(results_list, ignore_index=True)\n",
    "\n",
    "    # # Remove any player with the team equal to 'FA'\n",
    "    # final_results_df = final_results_df.query(\"TEAM != 'FA'\")\n",
    "\n",
    "    # # Save the results to a CSV file\n",
    "    # file_name = f\"predictions/LSTM_predictions_{pos}.csv\"\n",
    "    # final_results_df.to_csv(file_name, index=False)\n",
    "\n",
    "\n",
    "# predictions_df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
